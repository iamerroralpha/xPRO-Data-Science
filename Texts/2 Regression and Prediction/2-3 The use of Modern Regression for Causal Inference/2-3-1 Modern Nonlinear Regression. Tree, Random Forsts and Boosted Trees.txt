We begin a new block of our module devoted modern nonlinear regression.
Here we are interested in predicting the outcome y using the raw regressor Zed
which are k-dimensional.
Recall that the best prediction rule g(Z) is the conditional expectation of y
given Z.
In this module so far, we have used best linear prediction rules to approximate
g(Z) and linear regression or lost regression for estimation.
Now we consider nonlinear prediction rules to approximate g(Z),
including tree-based methods and neural networks.
In this segment we discussed tree-based methods.
The idea of regression trees is to partition the regressor space where Z
takes values into a set of rectangles, and
then for each rectangle provide a predicted value.
Suppose we have n observations, Zi, Yi for i ranging from 1 to n.
Given the partition of the space into M rectangles,
R1 through Rm, which will be determined by data.
The regression rule is a prediction of the four.
Hat g(Z) is equal to the sum over m from 1 to
m of hat beta m times the indicator that Z belongs to the rectangle Rm.
The estimated coefficients are obtained by minimizing the sample MSE,
as shown in this formula.
From the formula we can conclude that hat Beta m is set equal to average of Yi
with Zi falling in the rectangle Rm, as shown in the formula that you see.
The rectangles or regions Rm are called nodes and
each nodes has predicted value had Rm associated with it.
And nice thing about regression trees is that you get to draw cool
pictures like this one.
Here we show a tree based prediction rule for our wage example where Y is wage and
Z are experience, geographic, and educational characteristics.
As you can see from looking at the terminal nodes of the tree, in this tree,
the predicted hourly wage for college graduates with
more than 9.5 years of experience is $24, and otherwise it is 17.
The predicted wage for non-college graduates with more than 14 years of
experience is 14, and otherwise it is 12.
Now how do we grow this tree?
To make computation tractable, we use the recursive binary partitioning or
splitting of the regressor space.
First, we cut that regressor space into two regions by choosing a regressor and
a split point that achieve the best improvement in the sample MSE.
This gives us the tree of depth one, that you see in the figure.
The best variable to split on here is the indicator of college degree and
it takes values of 0 or 1, so the natural split point is 0.5.
This gives us a starting tree-based prediction rule,
which predicts a $20 hourly wage for college graduates and 13 for all others.
Second we repeat this procedure over two resulting regions or
nodes, college graduates and non-college graduates,
obtaining in this step four new nodes that we see in this figure.
For college graduates the splitting rule that minimizes MSE
is the experience regressor at 9.5 years.
This refines the prediction rule for
graduates to $24 if experience is greater than 9.5 years and $12 otherwise.
For non-graduates the procedure works similarly.
Third, we repeat this procedure over each of the four nodes.
The tree of that three now has eight nodes.
We see that in the final level we are now splitting our gender indicator,
high school graduate indicator, and the south indicator.
Finally, we stop growing the tree when the desired depths of the tree is reached.
Or when the minimal number of observations per node,
called minimal node, size is reached.
We've now made several observations.
First, the deeper we grow the tree,
the better is our approximation to the regression function g(Z).
On the other hand, the deeper the tree,
the more noisy our estimate of g(Z) becomes, since there are fewer
observations per terminal node to estimate the predicted value for this node.
From a prediction point of view, we can try to find the right depth or
the right structure of the tree by cross-validation.
For example, in the wage example the tree of depth 2 performs better in terms of
cross-validated MSE than the trees of depths 3 or 1.
The process of cutting down the branches of the tree to improve
predictive performance is called Pruning The Tree.
However in practice pruning the tree often doesn't give
satisfactory performance because a single prune tree
provides a very crude approximation to the regression function g(Z).
A much more powerful and one use approach to improve simple regression trees
is to build the Random Forest.
The idea of Random Forest is to grow many different deep trees and
then average prediction rules based on them.
The trees are grown over different artificial data samples
generated by sampling randomly with replacement from the original data.
This way of creating artificial samples is called the bootstrap statistics.
The trees are growing deep to keep the approximation error low and
averaging is meant to reduce the noisiness of the individual trees.
Let us define the bootstrap method.
Each bootstrap sample is created by sampling from our data on pairs (Yi,
Zi) randomly with replacement, so some observations get drawn multiple times and
some don't get redrawn at all.
Given a bootstrap sample numbered by numbered by numeral b,
we build a tree-based prediction rule hat gb (Z).
We repeat the procedure capital B times in total and
then average the prediction rules, that result from each of the bootstrap samples.
So here we have hat g random forest (Z) equals 1 over B times the sum of
hat g sub B of Z over B, where b runs from one to capital B.
Using bootstrap here is an intuitive idea.
If we could have many independent copies of the data, we could
average the prediction rules obtained over these copies to reduce the noisiness.
Since we don't have such copies, we rely on bootstrap copies of the data.
The key underlying idea here is that the trees are grown deep to keep
the approximation error low and
averaging is meant to reduce the noisiness of the individual trees.
The procedure of averaging noisy prediction rules over the bootstrap
samples is called Bootstrap Aggregation or Bagging.
Finally I would like to know that what we discuss here is the simplest version
of the random forest, and there are many different modifications
aimed at improving predictive performance that we didn't discuss.
But I'm encouraging you to look at the course materials for
additional references that discuss random force and more detail.
Another approach to improve simple regression tree is by boosting.
The idea of boosting is that of request of fatigue where estimated tree-based
prediction rule, then we take the residuals and estimate another shallow
tree-based prediction rule for these residuals and so on.
Summing up the prediction rules for
the residuals gives us the prediction rule for the outcome.
Unlike in a random forest, we use shallow trees, rather than deep trees, to keep
the noise low and each step of boosting aims to reduce the approximation error.
In order to avoid overfitting and boosting, we can stop the procedure
once we don't improve the cross-validated mean square error.
Formerly, the boosting algorithm looks as follows.
In step 1, we initialize the residuals Ri = Yi, for i that runs from 1 to n.
In step 2,
we perform the following operation over index j running from 1 to capital J.
We fit a tree-based prediction rule,
gj(Z) to the data (Zi,Ri) with i from 1 to n and we have data residuals
as R1 equals previous Ri minus lambda times hat g sub j (Zi).
Finally in step 3, we output the boosted prediction rule.
Hat g(Z) equals sum over J of lambda hat g sub j(Z).
The capital J and lambda that you see here are the tuning parameters.
Representing the number of boosting steps, and the degree of updating the residuals.
In particular, lambda equals 1, gives us the full update.
We can choose j and lambda by cross validation.
So let us summarize.
We discussed the tree based prediction rules, and
ways to improve them by bagging or boosting.
Bagging is bootstrap aggregation of the prediction rules.
Bootstrap aggregation of deep regression trees gives us random forests.
Boosting uses recursive fitting of residuals by a sequence of tree-based
prediction models.
The sum of these prediction rules gives us the prediction for outcome.