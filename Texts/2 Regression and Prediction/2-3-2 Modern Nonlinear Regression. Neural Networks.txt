In this segment,
we consider a non linear regression method, based on neural networks.
As before, we are interested in predicting the outcome Y using the raw regressors Z,
what are K-dimensional.
Recall that the best prediction rule of Y given Z is the function g(Z),
the conditional expectation of Y given Z.
The idea of the neural network is to use parameterized nonlinear transformations of
linear combinations of the raw regressors as constructed regressors, called neurons.
And produce the predicted value as a linear function of these regressors.
The prediction rule is g(Z) is nonlinear in some parameters and
with respect to raw regressors.
With sufficient many neurons neurons,
g(Z) can approximate the best prediction rule g(Z).
In part 2 of our module, we already saw that many constructed regressors
are useful in the high-dimensional linear setting to approximate g(Z).
Neural networks also rely on many constructed regressors to approximate
g(Z).
The method and the name neural networks were loosely inspired by the mode of
operation of the human brain, and
developed by the scientists working on Artificial Intelligence.
Accordingly, neural networks can be represented by cool graphs and
diagrams that we will discuss shortly, so please stay tuned.
To discuss the idea in more detail, we focus on the single layer neural network.
The estimated prediction rule will take the form hat g(Z) equals sum over M,
running from 1 to capital M of hat betaM times Xm of hat alphaM.
Where the Xm of hat alpha m's are constructed regressors called neurons.
The capital M neurons in total.
The neurons are generated by the formula Xm of alpha m = sigma of alpha mZ.
Where alpha m's are neuron-specific vectors of parameters called weights and
sigma is the activation function.
For example, sigma can be the sigmoid function given by this formula.
Or it can be the so
called rectified linear unit function or ReLU given by this formula.
The following figure shows the two graphs of the two Activation Functions.
The sigmoid function and the rectified linear unit function.
The horizontal axis shows the value of the argument.
And the vertical axis the value of the function.
The estimators had alpha m, and beta m for each M are obtained as the solution to
the penalized nonlinear least squares problem shown by this formula.
Here, we are minimizing the sum of squared prediction erros in the sample,
plus a penalty term given by the sum of the absolute values
of components of alpha m and beta m, multiplied by the penalty level, lambda.
In this formula, we use the lasso type penalty but
we can also use the ridge, another type of penalties.
The estimates are computed using sophisticated gradient descent algorithms.
Where sophistication is needed because nonlinear least squares optimization
problem is generally not a convex problem.
Making the computation a difficult task.
The procedural fetching renewal network model has tuning parameters and
in practice we can choose them by cross-validation.
The most important choices concern the number of neurons and
the number of neuron layers.
Having more neurons gives us more flexibility,
just like having more constructed regressors gave us more flexibility
with high-dimensional linear models.
To prevent overfitting,
we can rely on penalization as in the case of linear regression.
In order to visualize working of the neural network,
we rely on the resource called Playground.TensorFlow.org using which we
produce a prediction model given by simple single layer neural network model.
We now see the graphical representation of this network.
Here we have a regression problem and the network depicts the process of taking
row regressors and transforming them into predicted values.
In the second column on the left, we see the inputs are features.
These features are our two row regressors.
The third column shows eight neurons.
The neurons are constructed as linear combinations of the row
regressors transformed by an activation function.
That is, the neurons are along linear transformations of the row regressors.
Here, we set the activation function, to be the rectified linear unit function,
RE of U.
The neurons are connected to the inputs and
the connections represent the coefficients hat alpha M, which are coefficients
of the neuron specific linear transformations of raw regressors.
The coloring represents the sign or the coefficients, orange negative and
the blue positive.
And with all the connections represents the size of the coefficients.
The neurons are then linearly combined to produce the output, the prediction rule.
In the diagram we see the connections going outwards
from the neurons to the output.
These connections represent the coefficients hat beta M, or
the linear combination of the neurons that produce the final output.
The coloring and the widths represent the sign and the size of these coefficients.
In the diagram,
the prediction rule is shown by the heatmap in the box on the right.
On the horizontal and vertical axis, we see the values of the two inputs.
The color and its intensity in the heatmap represent the predicted value.
We also see on the top that the penalty function is L1, which stands for
the lasso type penalty.
Another option is to use L2, which stands for the rich type penalty.
The penalty level is called here the regularization rate.
In this example we are using a single layer neural network.
If we add one or
two additional layers of neurons constructed from the previous layer of
neurons, we get a different network, which we show in the following diagram.
Prediction methods based on neural networks with several layers of neurons
called the deep learning methods.
This diagrams showcases that one of the major benefits of doing prediction with
neural networks is that you can adapt with pretty cool artwork.
Let us summarize, in this segment we have discussed neural networks that have
recently gathered much attention under the umbrella of deep learning.
Neural networks represent a powerful and all-purpose method for
prediction and regression analysis.
Using many neurons and multiple layers gives rise to networks that are very
flexible and can approximate the best prediction rules quiet well.